{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 3 - Lab 2: Refactoring & Documentation (Solution)\n",
    "\n",
    "**Objective:** Use an LLM to refactor a complex Python function to improve its readability and maintainability, and then generate comprehensive, high-quality documentation for the project.\n",
    "\n",
    "**Introduction:**\n",
    "This solution notebook provides the complete prompts for the refactoring and documentation lab. It demonstrates how to guide an LLM to perform specific code quality improvements and generate structured documentation from multiple sources.\n",
    "\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 10:21:45,798 ag_aisoftdev.utils INFO LLM Client configured provider=huggingface model=deepseek-ai/DeepSeek-V3.1 latency_ms=None artifacts_path=None\n",
      "2025-10-30 10:21:48,258 ag_aisoftdev.utils INFO LLM Client configured provider=google model=gemini-2.5-pro latency_ms=None artifacts_path=None\n",
      "2025-10-30 10:21:48,783 ag_aisoftdev.utils INFO LLM Client configured provider=openai model=gpt-5-2025-08-07 latency_ms=None artifacts_path=None\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project's root directory to the Python path to ensure 'utils' can be imported.\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from utils import setup_llm_client, get_completion, save_artifact, load_artifact, clean_llm_output, prompt_enhancer\n",
    "\n",
    "# Initialize separate LLM clients for each task so we can pick recent models from different providers.\n",
    "# - Refactoring: strong instruction-following model\n",
    "# - Docstrings: model tuned for clarity/style\n",
    "# - README generation: high-capacity synthesis model\n",
    "refactor_client, refactor_model_name, refactor_api_provider = setup_llm_client(model_name=\"deepseek-ai/DeepSeek-V3.1\")\n",
    "doc_client, doc_model_name, doc_api_provider = setup_llm_client(model_name=\"gemini-2.5-pro\")\n",
    "readme_client, readme_model_name, readme_api_provider = setup_llm_client(model_name=\"gpt-5-2025-08-07\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The Code to Improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_code = \"\"\"\n",
    "def process_data(data, operation):\n",
    "    if operation == 'sum':\n",
    "        total = 0\n",
    "        for i in data:\n",
    "            total += i\n",
    "        return total\n",
    "    elif operation == 'average':\n",
    "        total = 0\n",
    "        for i in data:\n",
    "            total += i\n",
    "        return total / len(data)\n",
    "    elif operation == 'max':\n",
    "        max_val = data[0]\n",
    "        for i in data:\n",
    "            if i > max_val:\n",
    "                max_val = i\n",
    "        return max_val\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: The Challenges - Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): Refactoring the Code\n",
    "\n",
    "**Explanation:**\n",
    "This prompt is highly specific about the desired outcome. Instead of just saying \"improve this code,\" we give the LLM concrete principles to follow: apply the 'Single Responsibility Principle,' use built-in functions, and add type hints. This guidance transforms a vague request into a precise engineering task, leading to a much higher-quality output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 10:22:48,597 ag_aisoftdev.utils INFO LLM Client configured provider=openai model=o3 latency_ms=None artifacts_path=None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Refactoring Code ---\n",
      "from typing import Iterable, Union, Callable\n",
      "from statistics import mean\n",
      "\n",
      "Number = Union[int, float]\n",
      "\n",
      "def calculate_sum(data: Iterable[Number]) -> Number:\n",
      "    \"\"\"Calculate the sum of all elements in the input data.\"\"\"\n",
      "    return sum(data)\n",
      "\n",
      "def calculate_average(data: Iterable[Number]) -> float:\n",
      "    \"\"\"Calculate the arithmetic mean of the input data.\"\"\"\n",
      "    return mean(data)\n",
      "\n",
      "def calculate_max(data: Iterable[Number]) -> Number:\n",
      "    \"\"\"Find the maximum value in the input data.\"\"\"\n",
      "    return max(data)\n",
      "\n",
      "def process_data(data: Iterable[Number], operation: str) -> Union[Number, float]:\n",
      "    \"\"\"Process data using the specified operation.\n",
      "    \n",
      "    Args:\n",
      "        data: Iterable of numbers to process\n",
      "        operation: Operation to perform ('sum', 'average', or 'max')\n",
      "    \n",
      "    Returns:\n",
      "        Result of the specified operation\n",
      "        \n",
      "    Raises:\n",
      "        ValueError: If an invalid operation is specified\n",
      "    \"\"\"\n",
      "    operation_map: dict[str, Callable[[Iterable[Number]], Union[Number, float]]] = {\n",
      "        'sum': calculate_sum,\n",
      "        'average': calculate_average,\n",
      "        'max': calculate_max\n",
      "    }\n",
      "    \n",
      "    if operation not in operation_map:\n",
      "        raise ValueError(f\"Invalid operation '{operation}'. Must be one of: {list(operation_map.keys())}\")\n",
      "    \n",
      "    return operation_map[operation](data)\n"
     ]
    }
   ],
   "source": [
    "refactor_prompt = f\"\"\"\n",
    "You are a senior Python developer who writes clean, efficient, and maintainable code.\n",
    "\n",
    "Please refactor the following Python code. Apply the 'Single Responsibility Principle' by breaking the main function into smaller, more focused functions. Also, use modern Python features like built-in functions and add type hints.\n",
    "\n",
    "**Code to Refactor:**\n",
    "```python\n",
    "{bad_code}\n",
    "```\n",
    "\n",
    "Output only the refactored Python code.\n",
    "\"\"\"\n",
    "\n",
    "# Enhance the prompt for better results and consistency with other labs\n",
    "enhanced_refactor_prompt = prompt_enhancer(refactor_prompt)\n",
    "\n",
    "print(\"--- Refactoring Code ---\")\n",
    "refactored_code = get_completion(enhanced_refactor_prompt, refactor_client, refactor_model_name, refactor_api_provider)\n",
    "cleaned_code = clean_llm_output(refactored_code, language='python')\n",
    "print(cleaned_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): Generating Docstrings\n",
    "\n",
    "**Explanation:**\n",
    "This prompt builds on the previous step. We provide the newly refactored code and ask for another specific, structured output: Google-style docstrings. LLMs are exceptionally good at this type of structured text generation. They can parse the function signature to identify the arguments and return types and generate well-formatted, descriptive documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 11:08:29,601 ag_aisoftdev.utils INFO LLM Client configured provider=openai model=o3 latency_ms=None artifacts_path=None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Docstrings ---\n",
      "from typing import Iterable, Union, Callable\n",
      "from statistics import mean\n",
      "\n",
      "Number = Union[int, float]\n",
      "\n",
      "def calculate_sum(data: Iterable[Number]) -> Number:\n",
      "    \"\"\"Calculate the sum of all elements in an iterable.\n",
      "\n",
      "    Args:\n",
      "        data (Iterable[Number]): An iterable of numbers to be summed.\n",
      "\n",
      "    Returns:\n",
      "        Number: The sum of all numbers in the data.\n",
      "    \"\"\"\n",
      "    return sum(data)\n",
      "\n",
      "def calculate_average(data: Iterable[Number]) -> float:\n",
      "    \"\"\"Calculate the arithmetic mean of an iterable of numbers.\n",
      "\n",
      "    Args:\n",
      "        data (Iterable[Number]): An iterable of numbers to be averaged.\n",
      "\n",
      "    Returns:\n",
      "        float: The arithmetic mean of the numbers.\n",
      "    \"\"\"\n",
      "    return mean(data)\n",
      "\n",
      "def calculate_max(data: Iterable[Number]) -> Number:\n",
      "    \"\"\"Find the maximum value in an iterable of numbers.\n",
      "\n",
      "    Args:\n",
      "        data (Iterable[Number]): An iterable of numbers to search.\n",
      "\n",
      "    Returns:\n",
      "        Number: The largest number in the data.\n",
      "    \"\"\"\n",
      "    return max(data)\n",
      "\n",
      "def process_data(data: Iterable[Number], operation: str) -> Union[Number, float]:\n",
      "    \"\"\"Process a collection of numbers using a specified operation.\n",
      "\n",
      "    Args:\n",
      "        data (Iterable[Number]): An iterable of numbers to process.\n",
      "        operation (str): The name of the operation to perform. Must be\n",
      "            one of 'sum', 'average', or 'max'.\n",
      "\n",
      "    Returns:\n",
      "        Union[Number, float]: The result of applying the specified\n",
      "            operation to the data.\n",
      "\n",
      "    Raises:\n",
      "        ValueError: If an invalid operation name is provided.\n",
      "    \"\"\"\n",
      "    operation_map: dict[str, Callable[[Iterable[Number]], Union[Number, float]]] = {\n",
      "        'sum': calculate_sum,\n",
      "        'average': calculate_average,\n",
      "        'max': calculate_max\n",
      "    }\n",
      "    \n",
      "    if operation not in operation_map:\n",
      "        raise ValueError(f\"Invalid operation '{operation}'. Must be one of: {list(operation_map.keys())}\")\n",
      "    \n",
      "    return operation_map[operation](data)\n"
     ]
    }
   ],
   "source": [
    "docstring_prompt = f\"\"\"\n",
    "You are a Python developer who writes excellent documentation.\n",
    "\n",
    "Add Google-style docstrings to the following Python code. Each docstring should include a description of the function, its arguments (Args:), and what it returns (Returns:).\n",
    "\n",
    "**Python Code:**\n",
    "```python\n",
    "{cleaned_code}\n",
    "```\n",
    "\n",
    "Output the complete Python code with the added docstrings.\n",
    "\"\"\"\n",
    "\n",
    "# Enhance the prompt to improve clarity and formatting\n",
    "enhanced_docstring_prompt = prompt_enhancer(docstring_prompt)\n",
    "\n",
    "print(\"--- Generating Docstrings ---\")\n",
    "code_with_docstrings = get_completion(enhanced_docstring_prompt, doc_client, doc_model_name, doc_api_provider)\n",
    "cleaned_code_with_docstrings = clean_llm_output(code_with_docstrings, language='python')\n",
    "print(cleaned_code_with_docstrings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): Generating a Project README\n",
    "\n",
    "**Explanation:**\n",
    "Using an LLM to generate docstrings and a README is a massive productivity boost. It excels at this structured writing task, freeing up the developer to focus on complex logic while still ensuring the project is well-documented and easy for others to understand. This prompt is a synthesis task. We provide the LLM with both high-level requirements (the PRD) and low-level implementation details (the API source code). The LLM's job is to merge these two sources of information into a single, comprehensive `README.md` file, complete with overviews, feature lists, and practical `curl` examples derived from the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 12:27:35,640 ag_aisoftdev.utils INFO LLM Client configured provider=openai model=o3 latency_ms=None artifacts_path=None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Project README ---\n",
      "bash\n",
      "# Example template for GET\n",
      "curl -sS http://localhost:8000/your-endpoint\n",
      "\n",
      "# Example template for POST (replace body with the endpoint's schema)\n",
      "curl -sS -X POST http://localhost:8000/your-endpoint \\\n",
      "  -H \"Content-Type: application/json\" \\\n",
      "  -d '{\n",
      "    \"exampleField\": \"value\"\n",
      "  }'\n"
     ]
    }
   ],
   "source": [
    "# Load the necessary context files\n",
    "prd_content = load_artifact(\"artifacts/day1_prd.md\")\n",
    "api_code = load_artifact(\"app/main.py\")\n",
    "\n",
    "readme_prompt = f\"\"\"\n",
    "You are a technical writer creating a README.md file for a new open-source project.\n",
    "\n",
    "Use the provided Product Requirements Document (PRD) for high-level context and the FastAPI source code for technical details.\n",
    "\n",
    "**PRD Context:**\n",
    "<prd>\n",
    "{prd_content}\n",
    "</prd>\n",
    "\n",
    "**API Source Code:**\n",
    "<code>\n",
    "{api_code}\n",
    "</code>\n",
    "\n",
    "Generate a complete README.md file with the following sections:\n",
    "- Project Title\n",
    "- Overview (Summarize the project's purpose from the PRD)\n",
    "- Features\n",
    "- API Endpoints (List the available endpoints and provide a `curl` example for each one, including the POST request with a JSON body)\n",
    "- Setup and Installation (Provide basic instructions on how to run the FastAPI app with uvicorn)\n",
    "\"\"\"\n",
    "\n",
    "# Use prompt enhancer and the readme-specific client for synthesis\n",
    "enhanced_readme_prompt = prompt_enhancer(readme_prompt)\n",
    "\n",
    "print(\"--- Generating Project README ---\")\n",
    "if prd_content and api_code:\n",
    "    readme_content = get_completion(enhanced_readme_prompt, readme_client, readme_model_name, readme_api_provider)\n",
    "    cleaned_readme = clean_llm_output(readme_content, language='markdown')\n",
    "    print(cleaned_readme)\n",
    "    save_artifact(cleaned_readme, \"README.md\", overwrite=True)\n",
    "else:\n",
    "    print(\"Skipping README generation because PRD or API code is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Well done! You have used an LLM to perform two of the most valuable code quality tasks: refactoring and documentation. You've seen how AI can help transform messy code into a clean, maintainable structure and how it can generate comprehensive documentation from high-level project artifacts and source code. These skills are a massive productivity multiplier for any development team.\n",
    "\n",
    "> **Key Takeaway:** LLMs excel at understanding and generating structured text, whether that structure is code or documentation. Providing a clear 'before' state (the bad code) and a clear goal (the refactoring principles) allows the AI to perform complex code transformation and documentation tasks efficiently."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

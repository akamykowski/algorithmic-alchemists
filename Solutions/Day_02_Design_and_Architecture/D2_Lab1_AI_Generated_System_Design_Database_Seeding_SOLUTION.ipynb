{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2 - Lab 1: AI-Generated System Design & Database Seeding (Solution)\n",
    "\n",
    "**Objective:** Use the PRD artifact from Day 1 to generate a detailed SQL database schema, create realistic seed data, and then use those outputs to create and seed a live, local database file.\n",
    "\n",
    "**Introduction:**\n",
    "This solution notebook provides the complete prompts and Python code for Day 2's first lab. It demonstrates the workflow of generating design artifacts (SQL schema, seed data) and then using code to create a physical database from them.\n",
    "\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "**Explanation:**\n",
    "We load the `day1_prd.md` artifact from Day 1. This document is the single source of truth for our project's requirements and provides the essential context for the LLM to generate a relevant and accurate database schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 09:49:09,738 ag_aisoftdev.utils INFO LLM Client configured provider=openai model=gpt-4o latency_ms=None artifacts_path=None\n",
      "2025-10-30 09:49:12,819 ag_aisoftdev.utils INFO LLM Client configured provider=google model=gemini-2.5-pro latency_ms=None artifacts_path=None\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import sqlite3\n",
    "\n",
    "# Add the project's root directory to the Python path to ensure 'utils' can be imported.\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from utils import setup_llm_client, get_completion, save_artifact, load_artifact, clean_llm_output, recommended_models_table, prompt_enhancer\n",
    "\n",
    "# Initialize separate LLM clients for different artifacts to use the latest models from different providers.\n",
    "# - Schema generation uses a strong instruction-following model\n",
    "# - Seed data generation uses a model tuned for data generation\n",
    "schema_client, schema_model_name, schema_api_provider = setup_llm_client(model_name=\"gpt-4o\")\n",
    "seed_client, seed_model_name, seed_api_provider = setup_llm_client(model_name=\"gemini-2.5-pro\")\n",
    "\n",
    "# Load the PRD from Day 1\n",
    "prd_content = load_artifact(\"day1_prd.md\")\n",
    "if not prd_content:\n",
    "    print(\"Warning: Could not load day1_prd.md. Lab may not function correctly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Model | Provider | Text | Vision | Image Gen | Image Edit | Audio Transcription | Context Window | Max Output Tokens |\n",
       "|---|---|---|---|---|---|---|---|---|\n",
       "| MiniMaxAI/MiniMax-M2 | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 128,000 | 8,192 |\n",
       "| Qwen/Qwen-Image | huggingface | ❌ | ❌ | ✅ | ❌ | ❌ | - | - |\n",
       "| Qwen/Qwen-Image-Edit | huggingface | ❌ | ❌ | ❌ | ✅ | ❌ | - | - |\n",
       "| black-forest-labs/FLUX.1-Kontext-dev | huggingface | ❌ | ❌ | ❌ | ✅ | ❌ | - | - |\n",
       "| claude-haiku-4-5-20251001 | anthropic | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 64,000 |\n",
       "| claude-opus-4-1-20250805 | anthropic | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 32,000 |\n",
       "| claude-sonnet-4-5-20250929 | anthropic | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 64,000 |\n",
       "| dall-e-3 | openai | ❌ | ❌ | ✅ | ❌ | ❌ | - | - |\n",
       "| deepseek-ai/DeepSeek-V3.1 | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 128,000 | 100,000 |\n",
       "| gemini-1.5-flash | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 8,192 |\n",
       "| gemini-1.5-pro | google | ✅ | ✅ | ❌ | ❌ | ❌ | 2,000,000 | 8,192 |\n",
       "| gemini-2.0-flash-exp | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 8,192 |\n",
       "| gemini-2.0-flash-preview-image-generation | google | ❌ | ❌ | ✅ | ✅ | ❌ | 32,000 | 8,192 |\n",
       "| gemini-2.5-flash | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 65,536 |\n",
       "| gemini-2.5-flash-image-preview | google | ❌ | ❌ | ✅ | ✅ | ❌ | 32,768 | 32,768 |\n",
       "| gemini-2.5-flash-lite | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 65,536 |\n",
       "| gemini-2.5-pro | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 65,536 |\n",
       "| gemini-live-2.5-flash-preview | google | ❌ | ❌ | ❌ | ❌ | ❌ | 1,048,576 | 8,192 |\n",
       "| gpt-4.1 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 32,768 |\n",
       "| gpt-4.1-mini | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 32,000 |\n",
       "| gpt-4.1-nano | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 32,000 |\n",
       "| gpt-4o | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 128,000 | 16,384 |\n",
       "| gpt-4o-mini | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 128,000 | 16,384 |\n",
       "| gpt-4o-mini-transcribe | openai | ❌ | ❌ | ❌ | ❌ | ✅ | - | - |\n",
       "| gpt-4o-transcribe | openai | ❌ | ❌ | ❌ | ❌ | ✅ | - | - |\n",
       "| gpt-5-2025-08-07 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 400,000 | 128,000 |\n",
       "| gpt-5-mini-2025-08-07 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 400,000 | 128,000 |\n",
       "| gpt-5-nano-2025-08-07 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 400,000 | 128,000 |\n",
       "| meta-llama/Llama-3.3-70B-Instruct | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 8,192 | 4,096 |\n",
       "| meta-llama/Llama-4-Maverick-17B-128E-Instruct | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 1,000,000 | 100,000 |\n",
       "| meta-llama/Llama-4-Scout-17B-16E-Instruct | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 10,000,000 | 100,000 |\n",
       "| mistralai/Mistral-7B-Instruct-v0.3 | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 32,768 | 8,192 |\n",
       "| o3 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\n",
       "| o4-mini | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\n",
       "| stabilityai/stable-diffusion-3.5-large | huggingface | ❌ | ❌ | ✅ | ❌ | ❌ | - | - |\n",
       "| tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.5 | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 4,096 | 1,024 |\n",
       "| veo-3.0-fast-generate-preview | google | ❌ | ❌ | ❌ | ❌ | ❌ | 1,024 | - |\n",
       "| veo-3.0-generate-preview | google | ❌ | ❌ | ❌ | ❌ | ❌ | 1,024 | - |\n",
       "| whisper-1 | openai | ❌ | ❌ | ❌ | ❌ | ✅ | - | - |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'| Model | Provider | Text | Vision | Image Gen | Image Edit | Audio Transcription | Context Window | Max Output Tokens |\\n|---|---|---|---|---|---|---|---|---|\\n| MiniMaxAI/MiniMax-M2 | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 128,000 | 8,192 |\\n| Qwen/Qwen-Image | huggingface | ❌ | ❌ | ✅ | ❌ | ❌ | - | - |\\n| Qwen/Qwen-Image-Edit | huggingface | ❌ | ❌ | ❌ | ✅ | ❌ | - | - |\\n| black-forest-labs/FLUX.1-Kontext-dev | huggingface | ❌ | ❌ | ❌ | ✅ | ❌ | - | - |\\n| claude-haiku-4-5-20251001 | anthropic | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 64,000 |\\n| claude-opus-4-1-20250805 | anthropic | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 32,000 |\\n| claude-sonnet-4-5-20250929 | anthropic | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 64,000 |\\n| dall-e-3 | openai | ❌ | ❌ | ✅ | ❌ | ❌ | - | - |\\n| deepseek-ai/DeepSeek-V3.1 | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 128,000 | 100,000 |\\n| gemini-1.5-flash | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 8,192 |\\n| gemini-1.5-pro | google | ✅ | ✅ | ❌ | ❌ | ❌ | 2,000,000 | 8,192 |\\n| gemini-2.0-flash-exp | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 8,192 |\\n| gemini-2.0-flash-preview-image-generation | google | ❌ | ❌ | ✅ | ✅ | ❌ | 32,000 | 8,192 |\\n| gemini-2.5-flash | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 65,536 |\\n| gemini-2.5-flash-image-preview | google | ❌ | ❌ | ✅ | ✅ | ❌ | 32,768 | 32,768 |\\n| gemini-2.5-flash-lite | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 65,536 |\\n| gemini-2.5-pro | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 65,536 |\\n| gemini-live-2.5-flash-preview | google | ❌ | ❌ | ❌ | ❌ | ❌ | 1,048,576 | 8,192 |\\n| gpt-4.1 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 32,768 |\\n| gpt-4.1-mini | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 32,000 |\\n| gpt-4.1-nano | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 32,000 |\\n| gpt-4o | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 128,000 | 16,384 |\\n| gpt-4o-mini | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 128,000 | 16,384 |\\n| gpt-4o-mini-transcribe | openai | ❌ | ❌ | ❌ | ❌ | ✅ | - | - |\\n| gpt-4o-transcribe | openai | ❌ | ❌ | ❌ | ❌ | ✅ | - | - |\\n| gpt-5-2025-08-07 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 400,000 | 128,000 |\\n| gpt-5-mini-2025-08-07 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 400,000 | 128,000 |\\n| gpt-5-nano-2025-08-07 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 400,000 | 128,000 |\\n| meta-llama/Llama-3.3-70B-Instruct | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 8,192 | 4,096 |\\n| meta-llama/Llama-4-Maverick-17B-128E-Instruct | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 1,000,000 | 100,000 |\\n| meta-llama/Llama-4-Scout-17B-16E-Instruct | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 10,000,000 | 100,000 |\\n| mistralai/Mistral-7B-Instruct-v0.3 | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 32,768 | 8,192 |\\n| o3 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\\n| o4-mini | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\\n| stabilityai/stable-diffusion-3.5-large | huggingface | ❌ | ❌ | ✅ | ❌ | ❌ | - | - |\\n| tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.5 | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 4,096 | 1,024 |\\n| veo-3.0-fast-generate-preview | google | ❌ | ❌ | ❌ | ❌ | ❌ | 1,024 | - |\\n| veo-3.0-generate-preview | google | ❌ | ❌ | ❌ | ❌ | ❌ | 1,024 | - |\\n| whisper-1 | openai | ❌ | ❌ | ❌ | ❌ | ✅ | - | - |'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommended_models_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The Challenges - Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): Generating the SQL Schema\n",
    "\n",
    "**Explanation:**\n",
    "This prompt instructs the LLM to act as a Database Administrator (DBA). By providing the full PRD as context, we enable the LLM to understand the entities and relationships required by the application. The prompt specifically asks for `CREATE TABLE` statements, guiding the LLM to produce a ready-to-use SQL script. We then clean up the response to remove markdown fences and save the pure SQL code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 09:49:12,850 ag_aisoftdev.utils INFO LLM Client configured provider=openai model=o3 latency_ms=None artifacts_path=None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating SQL Schema ---\n",
      "Schema Enhanced prompt\n",
      " <prompt>\n",
      "  <persona>\n",
      "    You are an expert Database Administrator (DBA) and relational-database architect with 15 + years of experience designing normalized schemas for HR and onboarding SaaS platforms.\n",
      "  </persona>\n",
      "\n",
      "  <context>\n",
      "    Below is the full Product Requirements Document (PRD) for “OnboardPro,” an employee-onboarding application. Use it as the functional and business context for your design.\n",
      "\n",
      "    <prd>\n",
      "    # Product Requirements Document: OnboardPro\n",
      "    | Status | Draft |\n",
      "    | **Author** | Product Team – Onboarding |\n",
      "    | **Version** | 1.0 |\n",
      "    | **Last Updated** | 2023-10-27 |\n",
      "    … (full PRD text as provided above) …\n",
      "    </prd>\n",
      "\n",
      "    Key database scope for this task:\n",
      "    • You must create a normalized schema for a SQLite database.  \n",
      "    • The schema must include at least two tables:  \n",
      "      – users (id, name, email, role)  \n",
      "      – onboarding_tasks (id, title, description, due_date, status, user_id)  \n",
      "    • onboarding_tasks.user_id must be a foreign key referencing users.id.  \n",
      "    • Any additional tables or constraints that improve data integrity are allowed, but not required.  \n",
      "    • Follow SQLite conventions (e.g., INTEGER PRIMARY KEY, TEXT, DATE, etc.).\n",
      "  </context>\n",
      "\n",
      "  <instructions>\n",
      "    Think step-by-step about the entities, their attributes, and relationships to ensure third-normal-form (3NF) compliance.  \n",
      "    When you are ready, output the final SQL schema.\n",
      "\n",
      "    Important rules:  \n",
      "    1. Output ONLY raw SQL CREATE TABLE statements.  \n",
      "    2. Do NOT include explanations, comments, or any other text.  \n",
      "    3. Ensure foreign-key constraints use “REFERENCES users(id)”.\n",
      "  </instructions>\n",
      "\n",
      "  <output_format>\n",
      "    Raw SQL `CREATE TABLE` statements only.\n",
      "  </output_format>\n",
      "</prompt>\n",
      "CREATE TABLE users (\n",
      "    id INTEGER PRIMARY KEY,\n",
      "    name TEXT NOT NULL,\n",
      "    email TEXT NOT NULL UNIQUE,\n",
      "    role TEXT NOT NULL\n",
      ");\n",
      "\n",
      "CREATE TABLE onboarding_tasks (\n",
      "    id INTEGER PRIMARY KEY,\n",
      "    title TEXT NOT NULL,\n",
      "    description TEXT,\n",
      "    due_date DATE,\n",
      "    status TEXT NOT NULL,\n",
      "    user_id INTEGER,\n",
      "    FOREIGN KEY (user_id) REFERENCES users(id)\n",
      ");\n"
     ]
    }
   ],
   "source": [
    "schema_prompt = f\"\"\"\n",
    "You are an expert Database Administrator (DBA).\n",
    "\n",
    "Based on the following Product Requirements Document (PRD), design a normalized SQL schema for a SQLite database. The schema should include tables for users and their assigned onboarding tasks.\n",
    "\n",
    "**PRD Context:**\n",
    "<prd>\n",
    "{prd_content}\n",
    "</prd>\n",
    "\n",
    "The schema should have at least a `users` table and an `onboarding_tasks` table with a foreign key relationship.\n",
    "- The `users` table should include an id, name, email, and role (e.g., 'New Hire', 'Manager').\n",
    "- The `onboarding_tasks` table should include an id, a title, a description, a due_date, a status (e.g., 'Pending', 'Completed'), and a user_id foreign key.\n",
    "\n",
    "Output only the raw SQL `CREATE TABLE` statements.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating SQL Schema ---\")\n",
    "if prd_content:\n",
    "    # Enhance the raw schema prompt using the project's prompt enhancer\n",
    "    enhanced_schema_prompt = prompt_enhancer(schema_prompt)\n",
    "    print(\"Schema Enhanced prompt\\n\", enhanced_schema_prompt)\n",
    "\n",
    "    # Send the enhanced prompt to the schema-specific LLM client\n",
    "    generated_schema = get_completion(enhanced_schema_prompt, schema_client, schema_model_name, schema_api_provider)\n",
    "\n",
    "    # Clean up the generated schema\n",
    "    cleaned_schema = clean_llm_output(generated_schema, language='sql')\n",
    "    print(cleaned_schema)\n",
    "\n",
    "    # Save the cleaned schema to a file\n",
    "    save_artifact(cleaned_schema, \"artifacts/schema.sql\", overwrite=True)\n",
    "else:\n",
    "    print(\"Skipping schema generation because PRD is missing.\")\n",
    "    cleaned_schema = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): Generating Realistic Seed Data\n",
    "\n",
    "**Explanation:**\n",
    "An empty database isn't very useful for development. This prompt asks the LLM to generate realistic seed data. By providing both the PRD (for thematic context) and the SQL schema (for structural correctness), we guide the LLM to create `INSERT` statements that are both thematically appropriate (e.g., onboarding-related task titles) and syntactically correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 09:49:29,725 ag_aisoftdev.utils INFO LLM Client configured provider=openai model=o3 latency_ms=None artifacts_path=None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Seed Data ---\n",
      "Seed Data Enhanced prompt\n",
      " <prompt>\n",
      "    <persona>\n",
      "        You are a Senior Database Specialist and HR-Tech Domain Expert. Your job is to create realistic sample data for demos, POCs, and automated tests. You understand HR onboarding workflows, SQL best practices, and how to craft believable yet fictitious data that respects privacy regulations.\n",
      "    </persona>\n",
      "\n",
      "    <context>\n",
      "        Below is the full Product Requirements Document (PRD) for the “OnboardPro” employee-onboarding platform, followed by the SQL schema containing two tables—users and onboarding_tasks. Use this information to ground the sample data so that every record feels plausible within the product narrative.\n",
      "\n",
      "        ----------  PRD  ----------\n",
      "        # Product Requirements Document: OnboardPro\n",
      "        (full PRD text from the user’s message)\n",
      "\n",
      "        ----------  SQL SCHEMA ----------\n",
      "        CREATE TABLE users (\n",
      "            id INTEGER PRIMARY KEY,\n",
      "            name TEXT NOT NULL,\n",
      "            email TEXT NOT NULL UNIQUE,\n",
      "            role TEXT NOT NULL\n",
      "        );\n",
      "\n",
      "        CREATE TABLE onboarding_tasks (\n",
      "            id INTEGER PRIMARY KEY,\n",
      "            title TEXT NOT NULL,\n",
      "            description TEXT,\n",
      "            due_date DATE,\n",
      "            status TEXT NOT NULL,\n",
      "            user_id INTEGER,\n",
      "            FOREIGN KEY (user_id) REFERENCES users(id)\n",
      "        );\n",
      "    </context>\n",
      "\n",
      "    <instructions>\n",
      "        1. Think step-by-step (internally) to design believable sample data:\n",
      "           • Choose at least 3 distinct users that reflect personas in the PRD (e.g., HR Coordinator, Hiring Manager, New Hire, IT Admin).  \n",
      "           • Create 5–10 onboarding_tasks that mirror real tasks described in the PRD (e.g., “Complete I-9 Form,” “Schedule 1:1,” “Provision Laptop”).  \n",
      "           • Ensure each task is assigned to one of the created users (via user_id).  \n",
      "           • Use realistic but fictitious names, emails, dates (yyyy-mm-dd within the next 90 days), and statuses (“pending”, “completed”, etc.).  \n",
      "           • Maintain referential integrity: user_id in onboarding_tasks must exist in users.id.  \n",
      "           • Use sequential integer IDs starting at 1 for both tables.\n",
      "\n",
      "        2. Produce ONLY raw SQL INSERT statements—no commentary, no code fences, no additional whitespace above or below.  \n",
      "           • Insert 3–5 rows into users.  \n",
      "           • Insert 5–10 rows into onboarding_tasks.  \n",
      "\n",
      "        3. Do not output your chain-of-thought—only the final SQL.  \n",
      "    </instructions>\n",
      "\n",
      "    <output_format>\n",
      "        INSERT INTO users (id, name, email, role) VALUES (...);\n",
      "        INSERT INTO users (id, name, email, role) VALUES (...);\n",
      "        ...\n",
      "        INSERT INTO onboarding_tasks (id, title, description, due_date, status, user_id) VALUES (...);\n",
      "        ...\n",
      "    </output_format>\n",
      "</prompt>\n",
      "INSERT INTO users (id, name, email, role) VALUES (1, 'Priya Sharma', 'priya.sharma@examplecorp.com', 'New Hire');\n",
      "INSERT INTO users (id, name, email, role) VALUES (2, 'David Chen', 'david.chen@examplecorp.com', 'Hiring Manager');\n",
      "INSERT INTO users (id, name, email, role) VALUES (3, 'Maria Rodriguez', 'maria.rodriguez@examplecorp.com', 'HR Coordinator');\n",
      "INSERT INTO users (id, name, email, role) VALUES (4, 'Ben Carter', 'ben.carter@examplecorp.com', 'IT Admin');\n",
      "INSERT INTO onboarding_tasks (id, title, description, due_date, status, user_id) VALUES (1, 'Complete I-9 Form', 'Upload required identification documents and complete Section 1 of the Form I-9 for employment eligibility verification.', '2024-08-20', 'pending', 1);\n",
      "INSERT INTO onboarding_tasks (id, title, description, due_date, status, user_id) VALUES (2, 'Review Employee Handbook', 'Read the company''s employee handbook and acknowledge receipt in the HR portal.', '2024-08-25', 'completed', 1);\n",
      "INSERT INTO onboarding_tasks (id, title, description, due_date, status, user_id) VALUES (3, 'Provision Laptop and Peripherals', 'Configure and ship a standard developer laptop, monitor, keyboard, and mouse to Priya Sharma.', '2024-08-15', 'completed', 4);\n",
      "INSERT INTO onboarding_tasks (id, title, description, due_date, status, user_id) VALUES (4, 'Create System Accounts', 'Create accounts for Priya Sharma in Google Workspace, Slack, Jira, and GitHub.', '2024-08-15', 'completed', 4);\n",
      "INSERT INTO onboarding_tasks (id, title, description, due_date, status, user_id) VALUES (5, 'Schedule 30-Day Check-in', 'Set up a 1:1 meeting with Priya to discuss her progress and answer questions after her first month.', '2024-09-18', 'pending', 2);\n",
      "INSERT INTO onboarding_tasks (id, title, description, due_date, status, user_id) VALUES (6, 'Define First 90-Day Goals', 'Collaborate with Priya to establish clear, measurable goals for her first quarter.', '2024-09-02', 'in_progress', 2);\n",
      "INSERT INTO onboarding_tasks (id, title, description, due_date, status, user_id) VALUES (7, 'Verify I-9 Documentation', 'Review and verify the documents submitted by Priya Sharma for the Form I-9.', '2024-08-21', 'pending', 3);\n",
      "INSERT INTO onboarding_tasks (id, title, description, due_date, status, user_id) VALUES (8, 'Enroll in Benefits', 'Guide Priya Sharma through the benefits enrollment process and confirm her selections.', '2024-09-18', 'pending', 3);\n"
     ]
    }
   ],
   "source": [
    "seed_data_prompt = f\"\"\"\n",
    "You are a data specialist. Based on the provided PRD and SQL schema, generate 5-10 realistic SQL `INSERT` statements to populate the tables with sample data for an onboarding tool.\n",
    "\n",
    "**PRD Context:**\n",
    "<prd>\n",
    "{prd_content}\n",
    "</prd>\n",
    "\n",
    "**SQL Schema:**\n",
    "<schema>\n",
    "{cleaned_schema}\n",
    "</schema>\n",
    "\n",
    "Generate at least 3 users and 5 tasks assigned to those users.\n",
    "Output only the raw SQL `INSERT` statements.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating Seed Data ---\")\n",
    "if prd_content and cleaned_schema:\n",
    "    # Enhance the seed data prompt for better structure and fidelity\n",
    "    enhanced_seed_prompt = prompt_enhancer(seed_data_prompt)\n",
    "    print(\"Seed Data Enhanced prompt\\n\", enhanced_seed_prompt)\n",
    "\n",
    "    # Use the seed-data specific client\n",
    "    generated_seed_data = get_completion(enhanced_seed_prompt, seed_client, seed_model_name, seed_api_provider)\n",
    "\n",
    "    # Clean up the generated seed data\n",
    "    cleaned_seed_data = clean_llm_output(generated_seed_data, language='sql')\n",
    "    print(cleaned_seed_data)\n",
    "\n",
    "    # Save the cleaned seed data to a file\n",
    "    save_artifact(cleaned_seed_data, \"artifacts/seed_data.sql\", overwrite=True)\n",
    "else:\n",
    "    print(\"Skipping seed data generation because PRD or schema is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): Creating and Seeding a Live Database\n",
    "\n",
    "**Explanation:**\n",
    "This Python function demonstrates a crucial engineering task: turning text-based artifacts into a live system component. The `create_database` function uses Python's built-in `sqlite3` library.\n",
    "1.  It establishes a connection to a database file, which creates the file if it doesn't exist.\n",
    "2.  It reads the `schema.sql` artifact and executes it. It's important to use `cursor.executescript()` here. While `cursor.execute()` is designed for a single SQL statement, `executescript()` is necessary for running a string that contains multiple SQL statements, which is exactly what our `schema.sql` and `seed_data.sql` files contain.\n",
    "3.  It then reads and executes the `seed_data.sql` artifact to populate the newly created tables.\n",
    "4.  `conn.commit()` saves all the changes to the database file.\n",
    "5.  The `finally` block ensures that `conn.close()` is always called, which is a critical best practice to prevent resource leaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed existing database file at c:\\Workspace\\AG-AISOFTDEV\\artifacts\\onboarding.db\n",
      "Successfully connected to database at c:\\Workspace\\AG-AISOFTDEV\\artifacts\\onboarding.db\n",
      "Tables created successfully.\n",
      "Seed data inserted successfully.\n",
      "Database changes committed.\n",
      "Database connection closed.\n"
     ]
    }
   ],
   "source": [
    "def create_database(db_path, schema_path, seed_path):\n",
    "    \"\"\"Creates and seeds a SQLite database from SQL files.\"\"\"\n",
    "    if not os.path.exists(schema_path):\n",
    "        print(f\"Error: Schema file not found at {schema_path}\")\n",
    "        return\n",
    "\n",
    "    # Delete the old database file if it exists to start fresh\n",
    "    if os.path.exists(db_path):\n",
    "        os.remove(db_path)\n",
    "        print(f\"Removed existing database file at {db_path}\")\n",
    "\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cursor = conn.cursor()\n",
    "        print(f\"Successfully connected to database at {db_path}\")\n",
    "\n",
    "        # Read and execute the schema file\n",
    "        schema_sql = load_artifact(schema_path)\n",
    "        if schema_sql:\n",
    "            cursor.executescript(schema_sql)\n",
    "            print(\"Tables created successfully.\")\n",
    "\n",
    "        # Read and execute the seed data file if it exists\n",
    "        if os.path.exists(seed_path):\n",
    "            seed_sql = load_artifact(seed_path)\n",
    "            if seed_sql:\n",
    "                cursor.executescript(seed_sql)\n",
    "                print(\"Seed data inserted successfully.\")\n",
    "\n",
    "        conn.commit()\n",
    "        print(\"Database changes committed.\")\n",
    "\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Database error: {e}\")\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "            print(\"Database connection closed.\")\n",
    "\n",
    "# Define file paths\n",
    "db_file = os.path.join(project_root, \"artifacts\", \"onboarding.db\")\n",
    "schema_file = os.path.join(project_root, \"artifacts\", \"schema.sql\")\n",
    "seed_file = os.path.join(project_root, \"artifacts\", \"seed_data.sql\")\n",
    "\n",
    "# Execute the function\n",
    "create_database(db_file, schema_file, seed_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Excellent work! You have now moved from abstract requirements to a concrete, physical database artifact. You've used an LLM to design a schema, generate realistic test data, and then used a Python script to bring that database to life. This `onboarding.db` file is the foundation upon which we will build our API in Day 3.\n",
    "\n",
    "> **Key Takeaway:** The ability to generate structured data definitions (like a SQL schema) from unstructured text (like a PRD) is a core skill in AI-assisted development. It automates a critical and often time-consuming design step."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

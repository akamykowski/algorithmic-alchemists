{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 4 - Lab 1: Automated Testing & Quality Assurance\n",
    "\n",
    "**Objective:** Generate a comprehensive `pytest` test suite for the database-connected FastAPI application, including tests for happy paths, edge cases, and tests that use advanced fixtures for database isolation.\n",
    "\n",
    "**Estimated Time:** 135 minutes\n",
    "\n",
    "**Introduction:**\n",
    "Welcome to Day 4! An application without tests is an application that is broken by design. Today, we focus on quality assurance. You will act as a QA Engineer, using an AI co-pilot to build a robust test suite for the API you created yesterday. This is a critical step to ensure our application is reliable and ready for production.\n",
    "\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "We will load the source code for our main application from `app/main.py`. Providing the full code as context is essential for the LLM to generate accurate and relevant tests.\n",
    "\n",
    "**Model Selection:**\n",
    "For generating tests, models with strong code understanding and logical reasoning are best. `gpt-4.1`, `o3`, `codex-mini`, and `gemini-2.5-pro` are all excellent choices for this task.\n",
    "\n",
    "**Helper Functions Used:**\n",
    "- `setup_llm_client()`: To configure the API client.\n",
    "- `get_completion()`: To send prompts to the LLM.\n",
    "- `load_artifact()`: To read our application's source code.\n",
    "- `save_artifact()`: To save the generated test files.\n",
    "- `clean_llm_output()`: To clean up the generated Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-31 09:39:59,619 ag_aisoftdev.utils INFO LLM Client configured provider=google model=gemini-2.5-pro latency_ms=None artifacts_path=None\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project's root directory to the Python path to ensure 'utils' can be imported.\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from utils import setup_llm_client, get_completion, save_artifact, load_artifact, clean_llm_output\n",
    "\n",
    "client, model_name, api_provider = setup_llm_client(model_name=\"gemini-2.5-pro\")\n",
    "\n",
    "# Load the application code from Day 3 to provide context for test generation\n",
    "app_code = load_artifact(\"app/main.py\")\n",
    "if not app_code:\n",
    "    print(\"Warning: Could not load app/main.py. Lab may not function correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): Generating \"Happy Path\" Tests\n",
    "\n",
    "**Task:** Generate basic `pytest` tests for the ideal or \"happy path\" scenarios of your CRUD endpoints.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Create a prompt that asks the LLM to act as a QA Engineer.\n",
    "2.  Provide the `app_code` as context.\n",
    "3.  Instruct the LLM to generate a `pytest` test function for the `POST /users/` endpoint, asserting that a user is created successfully (e.g., checking for a `201 Created` or `200 OK` status code and verifying the response body).\n",
    "4.  Generate another test for the `GET /users/` endpoint.\n",
    "5.  Save the generated tests into a file named `tests/test_main_simple.py`.\n",
    "\n",
    "**Expected Quality:** A Python script containing valid `pytest` functions that test the basic, successful operation of your API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Happy Path Tests ---\n",
      "# tests/test_main.py\n",
      "\n",
      "import pytest\n",
      "from fastapi.testclient import TestClient\n",
      "from sqlalchemy import create_engine\n",
      "from sqlalchemy.orm import sessionmaker\n",
      "\n",
      "# Assuming the application code is in a file named 'main.py' inside an 'app' directory\n",
      "# and the db_models are in 'app/db_models.py'\n",
      "from app.main import app, get_db\n",
      "from app.db_models import Base\n",
      "\n",
      "# --- Test Database Setup ---\n",
      "# Use an in-memory SQLite database for testing to ensure tests are isolated and fast.\n",
      "# `connect_args` is needed for SQLite to allow multi-threaded access.\n",
      "SQLALCHEMY_DATABASE_URL = \"sqlite:///./test.db\"\n",
      "engine = create_engine(\n",
      "    SQLALCHEMY_DATABASE_URL, connect_args={\"check_same_thread\": False}\n",
      ")\n",
      "TestingSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n",
      "\n",
      "\n",
      "# --- Dependency Override ---\n",
      "# This function will override the `get_db` dependency in the main app.\n",
      "# Instead of connecting to the production database, requests will use this\n",
      "# in-memory test database session.\n",
      "def override_get_db():\n",
      "    try:\n",
      "        db = TestingSessionLocal()\n",
      "        yield db\n",
      "    finally:\n",
      "        db.close()\n",
      "\n",
      "\n",
      "# Apply the dependency override to the FastAPI app instance.\n",
      "app.dependency_overrides[get_db] = override_get_db\n",
      "\n",
      "\n",
      "# --- Pytest Fixture for Test Client ---\n",
      "# This fixture sets up the database before each test function runs and\n",
      "# cleans it up afterward, ensuring a clean state for every test.\n",
      "@pytest.fixture()\n",
      "def client():\n",
      "    # Create all database tables based on the SQLAlchemy models.\n",
      "    Base.metadata.create_all(bind=engine)\n",
      "    # Yield the TestClient instance to be used in the test functions.\n",
      "    yield TestClient(app)\n",
      "    # Drop all database tables after the test finishes to clean up.\n",
      "    Base.metadata.drop_all(bind=engine)\n",
      "\n",
      "\n",
      "# --- Test Functions ---\n",
      "\n",
      "def test_create_user(client: TestClient):\n",
      "    \"\"\"\n",
      "    Test for the POST /users/ endpoint.\n",
      "    It asserts that a new user can be created successfully by checking for\n",
      "    a 201 status code and verifying the response body.\n",
      "    \"\"\"\n",
      "    # 1. Define the payload for creating a new user.\n",
      "    user_data = {\"email\": \"test@example.com\", \"password\": \"a-secure-password\"}\n",
      "\n",
      "    # 2. Send a POST request to the /users/ endpoint with the user data.\n",
      "    response = client.post(\"/users/\", json=user_data)\n",
      "\n",
      "    # 3. Assert that the HTTP status code is 201 Created, indicating success.\n",
      "    assert response.status_code == 201, f\"Expected status code 201, but got {response.status_code}\"\n",
      "\n",
      "    # 4. Parse the JSON response body.\n",
      "    data = response.json()\n",
      "\n",
      "    # 5. Assert that the email in the response matches the one sent.\n",
      "    assert data[\"email\"] == user_data[\"email\"]\n",
      "    # 6. Assert that the response contains an 'id' key, confirming database insertion.\n",
      "    assert \"id\" in data\n",
      "\n",
      "\n",
      "def test_read_users(client: TestClient):\n",
      "    \"\"\"\n",
      "    Test for the GET /users/ endpoint.\n",
      "    It asserts that the endpoint returns a list of users with a 200 status code.\n",
      "    \"\"\"\n",
      "    # 1. Setup: Create a user first so the database is not empty.\n",
      "    user_data = {\"email\": \"testuser@example.com\", \"password\": \"password123\"}\n",
      "    client.post(\"/users/\", json=user_data)\n",
      "\n",
      "    # 2. Send a GET request to the /users/ endpoint to fetch all users.\n",
      "    response = client.get(\"/users/\")\n",
      "\n",
      "    # 3. Assert that the HTTP status code is 200 OK.\n",
      "    assert response.status_code == 200, f\"Expected status code 200, but got {response.status_code}\"\n",
      "\n",
      "    # 4. Parse the JSON response body.\n",
      "    data = response.json()\n",
      "\n",
      "    # 5. Assert that the response is a list.\n",
      "    assert isinstance(data, list)\n",
      "    # 6. Assert that the list is not empty, as we created a user.\n",
      "    assert len(data) > 0\n",
      "    # 7. Assert that the email of the user in the list matches the one we created.\n",
      "    assert data[0][\"email\"] == user_data[\"email\"]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Write a prompt to generate happy path tests for your API.\n",
    "happy_path_tests_prompt = f\"\"\"\n",
    "Act as a QA Engineer.\n",
    "The current application code is as follows:\n",
    "{app_code}\n",
    "Generate a pytest test function for the POST /users/ endpoint, asserting that a user is created successfully,\n",
    "checking for a 201 status code and correct response body.\n",
    "Generate another test for the GET /users/ endpoint, asserting that it returns a list of users with a 200 status code.\n",
    "Ensure that the tests are well-structured, use appropriate pytest fixtures if necessary, and include comments explaining each step.\n",
    "Don't include unnecessary comments or explanations outside the code block.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating Happy Path Tests ---\")\n",
    "if app_code:\n",
    "    generated_happy_path_tests = get_completion(happy_path_tests_prompt, client, model_name, api_provider)\n",
    "    cleaned_tests = clean_llm_output(generated_happy_path_tests, language='python')\n",
    "    print(cleaned_tests)\n",
    "    save_artifact(cleaned_tests, \"tests/test_main_simple.py\", overwrite=True)\n",
    "else:\n",
    "    print(\"Skipping test generation because app code is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): Generating Edge Case Tests\n",
    "\n",
    "**Task:** Prompt the LLM to generate tests for common edge cases, such as providing invalid data or requesting a non-existent resource.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Create a new prompt.\n",
    "2.  Provide the `app_code` as context.\n",
    "3.  Instruct the LLM to write two new test functions:\n",
    "    * A test for the `POST /users/` endpoint that tries to create a user with an email that already exists, asserting that the API returns a `400 Bad Request` error.\n",
    "    * A test for the `GET /users/{user_id}` endpoint that requests a non-existent user ID, asserting that the API returns a `404 Not Found` error.\n",
    "\n",
    "**Expected Quality:** Two new `pytest` functions that verify the application handles common error scenarios correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Edge Case Tests ---\n",
      "import pytest\n",
      "from fastapi.testclient import TestClient\n",
      "from sqlalchemy import create_engine\n",
      "from sqlalchemy.orm import sessionmaker\n",
      "from sqlalchemy.pool import StaticPool\n",
      "\n",
      "# Assuming the provided application code is in a file named `main.py`\n",
      "# and the SQLAlchemy models are in `db_models.py`\n",
      "from main import app, get_db, Base, User\n",
      "\n",
      "# --- Test Database Setup ---\n",
      "# Use an in-memory SQLite database for testing to ensure tests are fast and isolated.\n",
      "# `connect_args={\"check_same_thread\": False}` is needed for SQLite.\n",
      "# `StaticPool` is used to ensure the same connection is used for the lifespan of the test session.\n",
      "SQLALCHEMY_DATABASE_URL = \"sqlite:///:memory:\"\n",
      "\n",
      "engine = create_engine(\n",
      "    SQLALCHEMY_DATABASE_URL,\n",
      "    connect_args={\"check_same_thread\": False},\n",
      "    poolclass=StaticPool,\n",
      ")\n",
      "TestingSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n",
      "\n",
      "\n",
      "# --- Dependency Override ---\n",
      "# This function will override the `get_db` dependency in the main app.\n",
      "# Instead of connecting to the production database, API calls during tests\n",
      "# will use this connection to the in-memory test database.\n",
      "def override_get_db():\n",
      "    db = TestingSessionLocal()\n",
      "    try:\n",
      "        yield db\n",
      "    finally:\n",
      "        db.close()\n",
      "\n",
      "\n",
      "# Apply the dependency override to the FastAPI app instance.\n",
      "app.dependency_overrides[get_db] = override_get_db\n",
      "\n",
      "\n",
      "@pytest.fixture(scope=\"function\")\n",
      "def client():\n",
      "    \"\"\"\n",
      "    Pytest fixture to set up and tear down the test environment for each test function.\n",
      "    - Creates all database tables before a test runs.\n",
      "    - Yields a TestClient instance to interact with the API.\n",
      "    - Drops all database tables after the test completes to ensure isolation.\n",
      "    \"\"\"\n",
      "    # Create the tables in the in-memory database\n",
      "    Base.metadata.create_all(bind=engine)\n",
      "    # Yield the test client to the test function\n",
      "    yield TestClient(app)\n",
      "    # Drop all tables after the test is done\n",
      "    Base.metadata.drop_all(bind=engine)\n",
      "\n",
      "\n",
      "def test_create_user_with_existing_email(client: TestClient):\n",
      "    \"\"\"\n",
      "    Tests the POST /users/ endpoint for creating a user with a duplicate email.\n",
      "\n",
      "    Steps:\n",
      "    1. Define the user data for a new user.\n",
      "    2. Send a POST request to create the user for the first time.\n",
      "    3. Assert that the first creation is successful (201 Created).\n",
      "    4. Send another POST request with the exact same user data.\n",
      "    5. Assert that the API returns a 400 Bad Request status code.\n",
      "    6. Assert that the response body contains the appropriate error detail message.\n",
      "\n",
      "    NOTE: This test assumes the application logic is (or will be) updated to\n",
      "    check for existing emails and return a 400 error, which is standard practice.\n",
      "    The provided code snippet might need modification to make this test pass.\n",
      "    \"\"\"\n",
      "    # 1. Define user data to be used for creation.\n",
      "    user_data = {\"email\": \"test.duplicate@example.com\", \"password\": \"password123\"}\n",
      "\n",
      "    # 2. Create the user for the first time.\n",
      "    first_response = client.post(\"/users/\", json=user_data)\n",
      "    # 3. Assert the first creation was successful.\n",
      "    assert first_response.status_code == 201\n",
      "\n",
      "    # 4. Attempt to create the same user again.\n",
      "    second_response = client.post(\"/users/\", json=user_data)\n",
      "    # 5. Assert that the API correctly returns a 400 Bad Request status.\n",
      "    assert second_response.status_code == 400\n",
      "    # 6. Assert the error message is what's expected.\n",
      "    # The exact error message depends on the API's implementation.\n",
      "    # \"Email already registered\" is a common and clear message.\n",
      "    assert second_response.json() == {\"detail\": \"Email already registered\"}\n",
      "\n",
      "\n",
      "def test_read_non_existent_user(client: TestClient):\n",
      "    \"\"\"\n",
      "    Tests the GET /users/{user_id} endpoint for a user that does not exist.\n",
      "\n",
      "    Steps:\n",
      "    1. Choose a user ID that is highly unlikely to exist in the database (e.g., 999).\n",
      "    2. Send a GET request to the endpoint with this non-existent ID.\n",
      "    3. Assert that the API returns a 404 Not Found status code.\n",
      "    4. Assert that the response body contains the \"User not found\" detail message.\n",
      "    \"\"\"\n",
      "    # 1. Choose a non-existent user ID.\n",
      "    non_existent_user_id = 999\n",
      "\n",
      "    # 2. Send a GET request for this user.\n",
      "    response = client.get(f\"/users/{non_existent_user_id}\")\n",
      "\n",
      "    # 3. Assert the status code is 404 Not Found.\n",
      "    assert response.status_code == 404\n",
      "\n",
      "    # 4. Assert the detail message matches the one in the application's HTTPException.\n",
      "    assert response.json() == {\"detail\": \"User not found\"}\n"
     ]
    }
   ],
   "source": [
    "# TODO: Write a prompt to generate edge case tests.\n",
    "edge_case_tests_prompt = f\"\"\"\n",
    "Utilizing the same application code:\n",
    "{app_code}\n",
    "Generate one pytest test function for the POST /users/ endpoint that tries to create a user with an email that already exists, asserting that the API returns a 400 status code and an appropriate error message.\n",
    "Generate another test for the GET /users/user_id endpoint that requests a non-existent user ID, asserting that the API returns a 404 Not Found error.\n",
    "Ensure that the tests are well-structured, use appropriate pytest fixtures if necessary, and include comments explaining each step.\n",
    "Don't include unnecessary comments or explanations outside the code block.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating Edge Case Tests ---\")\n",
    "if app_code:\n",
    "    generated_edge_case_tests = get_completion(edge_case_tests_prompt, client, model_name, api_provider)\n",
    "    cleaned_edge_case_tests = clean_llm_output(generated_edge_case_tests, language='python')\n",
    "    print(cleaned_edge_case_tests)\n",
    "else:\n",
    "    print(\"Skipping test generation because app code is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): Testing with an Isolated Database Fixture\n",
    "\n",
    "**Task:** Generate a `pytest` fixture that creates a fresh, isolated, in-memory database for each test session. Then, refactor your tests to use this fixture. This is a critical pattern for professional-grade testing.\n",
    "\n",
    "> **Hint:** Why use an isolated database? Running tests against your actual development database can lead to data corruption and flaky, unreliable tests. A pytest fixture that creates a fresh, in-memory database for each test ensures that your tests are independent, repeatable, and have no side effects.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Create a prompt that asks the LLM to generate a `pytest` fixture.\n",
    "2.  This fixture should configure a temporary, in-memory SQLite database using SQLAlchemy.\n",
    "3.  It needs to create all the database tables before the test runs and tear them down afterward.\n",
    "4.  Crucially, it must override the `get_db` dependency in your FastAPI app to use this temporary database during tests.\n",
    "5.  Save the generated fixture code to a special file named `tests/conftest.py`.\n",
    "6.  Finally, create a new test file, `tests/test_main_with_fixture.py`, and ask the LLM to rewrite the happy-path tests from Challenge 1 to use the new database fixture.\n",
    "\n",
    "**Expected Quality:** Two new files, `tests/conftest.py` and `tests/test_main_with_fixture.py`, containing a professional `pytest` fixture for database isolation and tests that are correctly refactored to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write a prompt to generate the pytest fixture for an isolated test database.\n",
    "db_fixture_prompt = f\"\"\"\n",
    "Generate a pytest fixture.\n",
    "The fixture should configure a temporary, in-memory SQLite database using SQLAlchemy.\n",
    "It needs to create all the database tables before the test runs and tear them down afterward.\n",
    "Crucially, it must override the get_db dependency in \n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating Pytest DB Fixture ---\")\n",
    "if app_code:\n",
    "    generated_db_fixture = get_completion(db_fixture_prompt, client, model_name, api_provider)\n",
    "    cleaned_fixture = clean_llm_output(generated_db_fixture, language='python')\n",
    "    print(cleaned_fixture)\n",
    "    save_artifact(cleaned_fixture, \"tests/conftest.py\")\n",
    "else:\n",
    "    print(\"Skipping fixture generation because app context is missing.\")\n",
    "\n",
    "# TODO: Write a prompt to refactor the happy path tests to use the new fixture.\n",
    "refactor_tests_prompt = f\"\"\"\n",
    "# Your prompt here\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n--- Generating Refactored Tests ---\")\n",
    "if app_code:\n",
    "    refactored_tests = get_completion(refactor_tests_prompt, client, model_name, api_provider)\n",
    "    cleaned_refactored_tests = clean_llm_output(refactored_tests, language='python')\n",
    "    print(cleaned_refactored_tests)\n",
    "    save_artifact(cleaned_refactored_tests, \"tests/test_main_with_fixture.py\")\n",
    "else:\n",
    "    print(\"Skipping test refactoring because app context is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Fantastic work! You have built a comprehensive test suite for your API, moving from simple happy path tests to advanced, isolated database testing. You've learned how to use AI to brainstorm edge cases and generate complex fixtures. Having a strong test suite like this gives you the confidence to make changes to your application without fear of breaking existing functionality.\n",
    "\n",
    "> **Key Takeaway:** Using AI to generate tests is a massive force multiplier for quality assurance. It excels at creating boilerplate test code, brainstorming edge cases, and generating complex setup fixtures, allowing developers to build more reliable software faster."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
